{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Welcome to Bittensor!**\n",
    "\n",
    "(This notebook is supposed to run with pip-installed version of bittensor)\n",
    "\n",
    "Bittensor is a decentralized market that enables individuals to monetize intelligence production from any computer anywhere in the world. Intelligence production is validated by the other peers in the network and rewarded through token inflation. Consumers stake this currency to gain access to the produced knowledge. Bittensor is collectively-run, open-source, and open-access.\n",
    "\n",
    "For more info...\n",
    "\n",
    "* [Visit our website](https://www.bittensor.com/)\n",
    "* [Read our paper](https://arxiv.org/abs/2003.03917)\n",
    "* [Check out the code](https://github.com/opentensor/BitTensor)\n",
    "* [Talk to us!](https://discord.gg/3rUr6EcvbB)\n",
    "\n",
    "### **Some basic terminologies**\n",
    "Before moving forward, know these terms!\n",
    "\n",
    "1. Tau: The currency name.\n",
    "2. Neuron/node/peers/endpoint: Server that mines Tau, each server owns a machine learning model, which we call nucleus.\n",
    "3. Network/chain: The whole group of people that miners Tau.\n",
    "4. Weight: The scoring that other neurons in the network gives you. \n",
    "5. Stake: The higher the stake, the more Tau you own. It is also determined by your weight and the time you stay in the network.\n",
    "\n",
    "### **Structure of this tutorial**\n",
    "1. [Quick start!](#quickStart) \n",
    "3. [Wallet](#wallet)\n",
    "2. [Metagraph](#metagraph)\n",
    "6. [Subtensor](#subtensor)\n",
    "4. [Dendrite](#dendrite)\n",
    "5. [Axon](#axon)\n",
    "\n",
    "\n",
    "\n",
    "### **Let's get the bittensor package ready!**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- install the package ----\n",
    "! pip install bittensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- import ----\n",
    "import bittensor\n",
    "import torch\n",
    "\n",
    "# ---- Enable logging from bittensor ----\n",
    "bittensor.logging(debug = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='quickStart'></a>\n",
    "# **Quick start!** \n",
    "\n",
    "#### **Client side**\n",
    "\n",
    "On the client side, we can send request for a representation of our data from each of the chosen node."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- set up a wellet ----\n",
    "wallet = bittensor.wallet().create()\n",
    "\n",
    "# ---- sync to our metagraph (this will be further explained) ----\n",
    "graph = bittensor.metagraph().sync()\n",
    "\n",
    "# ---- querying for representation by calling dendrite ----\n",
    "representations, _ = bittensor.dendrite( wallet = wallet ).forward_text (\n",
    "    endpoints = graph.endpoints,\n",
    "    inputs = \"The quick brown fox jumped over the lazy dog\"\n",
    ")\n",
    "# ---- checking out the representations ----\n",
    "# representations = Tensor with shape (number of nodes online, 9, 512)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Server side\n",
    "\n",
    "On the server side, we get to mine Tau using our own model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# ---- set up a wallet ----\n",
    "wallet = bittensor.wallet().create()\n",
    "\n",
    "# ---- set up the forward function ----\n",
    "model = BertModel(BertConfig())\n",
    "\n",
    "def forward ( pubkey, inputs_x, modality ):\n",
    "    return model( inputs_x ).narrow(2, 0, bittensor.__network_dim__)\n",
    "\n",
    "# ---- subscribe to axon, that handles forward and backward request from other neurons ----\n",
    "axon = bittensor.axon (\n",
    "    wallet = wallet,\n",
    "    forward_text = forward,\n",
    ").start().subscribe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='wallet'></a>\n",
    "# **Wallet**\n",
    "\n",
    "A wallet has the following properties.\n",
    "* name\n",
    "* coldkey: Used to store, transfer, and stake tokens. It is \"cold\" because it is not loaded into the miner and remains encrypted on the device. \n",
    "* hotkey: Used by the miner to subscribe and set weights. It is \"hot\" because it is loaded into the running software (which can be insecure). It does not have permission to move funds.\n",
    "\n",
    "NOTE: Remember to save the mnemonic of the cold key and hot key for regenerating your password or hot key."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- initialize a wallet object ----\n",
    "# if you want to creat a new wallet, choose a new wallet name, else just choose an existing wallet name \n",
    "\n",
    "wallet = bittensor.wallet(name = 'wallet-name')\n",
    "wallet.create()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='metagraph'></a>\n",
    "# **Metagraph** \n",
    "Metagraph is an object that maintains the chain state as a torch.nn.Module. First, setp up the metagraph object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- Initializing the metagraph object ----\n",
    "meta = bittensor.metagraph()\n",
    "\n",
    "# ---- Synchronise states in meta graph with the akatsuki network, aka the chain----\n",
    "meta = meta.sync()\n",
    "\n",
    "print(\"---- Metagraph keys ----\\n\\n\", meta.state_dict().keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Block info**\n",
    "* n: the number of neurons in the network\n",
    "* block: the block id \n",
    "* tau (T): the token inflation rate per block"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"---- Block info ----\\n\")\n",
    "print(\"n:\\t\\t\", meta.n.item())\n",
    "print(\"block ID:\\t\", meta.block.item())\n",
    "print(\"tau:\\t\\t\", meta.tau[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Neurons' info**\n",
    "* uids: the user id of each neurons\n",
    "* last_update: the last time the the users emit to the chain\n",
    "* endpoints: the encoded ip addresses, hotkey, coldkey of other neurons. Can be retrieved by bittensor.endpoint.from_tensor()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"---- Neurons' info (of the first 5 neurons) ----\\n\")\n",
    "print(\"uids :\\t\", meta.uids[0:5])\n",
    "print(\"last_update :\\t\", meta.last_update[0:5])\n",
    "print(\"\\nendpoint (of the first neuron):\\n\\n\", meta.endpoints[0])\n",
    "print(\"\\nip address/hotkey/coldkey (of the first neuron): \\n\\n\", bittensor.endpoint.from_tensor(meta.endpoints[0]))\n",
    "\n",
    "print(\"\\nhotkeys: \\n\\n\", meta.hotkeys[0:5])\n",
    "print(\"\\ncoldkeys: \\n\\n\", meta.coldkeys[0:5])\n",
    "print(\"\\naddresses: \\n\\n\", meta.addresses[0:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Neuron's scorings**\n",
    "* weight (W): the weight of of the network, which is an n*n matrix. W_{i,j} indicates the locally calculated score from uid[i] to uid[j]. \n",
    "* rank (R): R = W<sup>T</sup> &middot; S. It also controls the priority of message to be read by peers.\n",
    "* trust (T): T = $\\sigma$ (C <sup>T</sup> S), where C is the connectivity matrix of size n*n. C<sub>i,j</sub> indicates the number of expectation in time that peer i reach peer j.\n",
    "* incentive (I): I = W<sup>T</sup> S &middot; $\\sigma$ (C <sup>T</sup> S)\n",
    "* stake (S): the stake of each neuron, updated with function S<sub>t+1</sub> = S<sub>t</sub> + (&tau; R ||S||) / ||R|| "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"---- Neurons' scoring (of and within the first 5 neurons) ----\\n\")\n",
    "print(\"Weight:\\n\", meta.W[0:5, 0:5])\n",
    "print(\"Stake:\\t\", meta.S[0:5])\n",
    "print(\"Rank:\\t\", meta.R[0:5])\n",
    "print(\"Incentive:\\t\", meta.I[0:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Saving and loading the metagraph** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "meta.save()\n",
    "meta.load()\n",
    "\n",
    "meta.save_to_path('~/.bittensor/', 'network-name.pt')\n",
    "meta.load_from_path('~/.bittensor/network-name.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Use Cases**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO: maybe list more use cases\n",
    "\n",
    "# ---- getting the endpoints with stake at the top 90% ----\n",
    "meta.endpoints[meta.S > torch.quantile(meta.S, 0.9)]\n",
    "\n",
    "# ---- querying the stake of a neuron from its pubkey(hotkey) ----\n",
    "# stake_of_caller = meta.S[meta.hotkeys.index(pubkey)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='subtensor'></a>\n",
    "# **Subtensor**\n",
    "\n",
    "It handles the interactions with the subtensor chain."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- initialize ----\n",
    "subtensor = bittensor.subtensor()\n",
    "\n",
    "# ---- Subscribes an bittensor endpoint to the substensor chain. ----\n",
    "subtensor.subscribe(wallet, axon.external_ip, axon.external_port, 0)\n",
    "\n",
    "# ---- shall we include these functions as well? ----\n",
    "\n",
    "# subtensor.connect()\n",
    "# subtensor.set_weights()\n",
    "# subtensor.get_balances()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='dendrite'></a>\n",
    "# **Dendrite**\n",
    "\n",
    "The dendrite object is for questing representation of your data from other neurons."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the projected size of representation\n",
    "bittensor.__network_dim__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- initializing a dendrite object ----\n",
    "# note: it is not required to attach a wallet to the dendrite object \n",
    "dend = bittensor.dendrite(wallet = wallet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- specify the endpoints that you want to send request to ----\n",
    "# here we select the top 10% endpoints that has the most stake \n",
    "endpoints = meta.endpoints[meta.S > torch.quantile(meta.S, 0.9)]\n",
    "\n",
    "# ---- retrieving the represenataions from other \n",
    "representation = dend.forward_text(endpoints, \"The quick brown fox jumped over the lazy dog\" )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- checking on the representation ----\n",
    "representation[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='axon'></a>\n",
    "# **Axon**\n",
    "\n",
    "An axon object that handles forward and backward request from other neurons."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- first, define a model and a forward function to be passed to the Axon object ----\n",
    "# the resulted model representation has to be narrowed to our network_dim \n",
    "\n",
    "from transformers import BertModel, BertConfig\n",
    "model = BertModel(BertConfig())\n",
    "\n",
    "def forward ( pubkey, inputs_x, modality ):\n",
    "    return model( inputs ).narrow(2, 0, bittensor.__network_dim__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- initialize the axon object ----\n",
    "# ---- start the axon object, but other neurons still can't find you!!! ----\n",
    "# ---- subscribe the axon object to the network. Okay! other neurons can see you now! ---- \n",
    "axon_ = bittensor.axon (\n",
    "    wallet = wallet,\n",
    "    forward_text = forward,\n",
    ").start().subscribe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bug?\n",
    "axon_ = bittensor.axon (\n",
    "    wallet = wallet,\n",
    "    forward_text = forward,\n",
    ")\n",
    "axon.start()\n",
    "axon.subscribe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ---- check your subscribed ip and port ----\n",
    "print(f\"{axon.external_ip}:{axon.external_port}\" )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bit",
   "language": "python",
   "name": "bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}